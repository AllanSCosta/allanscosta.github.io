---
---


@article {Lin2022.07.20.500902,
author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Santos Costa, Allan dos and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and Rives, Alexander},
title = {Language models of protein sequences at the scale of evolution enable accurate structure prediction},
elocation-id = {2022.07.20.500902},
year = {2022},
doi = {10.1101/2022.07.20.500902},
publisher = {Cold Spring Harbor Laboratory},
abstract = {Large language models have recently been shown to develop emergent capabilities with scale, going beyond simple pattern matching to perform higher level reasoning and generate lifelike images and text. While language models trained on protein sequences have been studied at a smaller scale, little is known about what they learn about biology as they are scaled up. In this work we train models up to 15 billion parameters, the largest language models of proteins to be evaluated to date. We find that as models are scaled they learn information enabling the prediction of the three-dimensional structure of a protein at the resolution of individual atoms. We present ESMFold for high accuracy end-to-end atomic level structure prediction directly from the individual sequence of a protein. ESMFold has similar accuracy to AlphaFold2 and RoseTTAFold for sequences with low perplexity that are well understood by the language model. ESMFold inference is an order of magnitude faster than AlphaFold2, enabling exploration of the structural space of metagenomic proteins in practical timescales.Competing Interest StatementThe authors have declared no competing interest.},
URL = {https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902},
eprint = {https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902.full.pdf},
journal = {bioRxiv}
}

@inproceedings{inproceedings,
author = {Liu, David and Melo, Lígia and Costa, Allan and Vögele, Martin and Townshend, Raphael and Dror, Ron},
year = {2022},
month = {07},
pages = {},
title = {Euclidean Transformers for Macromolecular Structures: Lessons Learned}
}


@inproceedings{inproceedings,
author = {Costa, Allan and Ponnapati, Manvitha and Alcaide, Eric and Palepu, Kalyan and Bhat, Suhaas and Chaterjee, Pranam and Jacobson, Joseph and Drori, Iddo},
booktitle   = {NeurIPS Workshop on Learning Meaningful Representations of Life (Poster)},
year = {2021},
month = {12},
pages = {},
title = {InterDocker: End-to-end cross-attentive and geometric Transformers for efficient iterative protein docking}
}


@inproceedings{alma9935262359806761,
abstract = {Determining the structure of proteins has been a long-standing goal in biology. Language models have been recently deployed to capture the evolutionary semantics of protein sequences, and as an emergent property, were found to be structural learners. Enriched with multiple sequence alignments (MSA), these transformer models were able to capture significant information about a protein's tertiary structure. In this work, we show how such structural information can be recovered by processing language model embeddings, and introduce a two-stage folding pipeline to directly estimate three-dimensional folded structures from protein sequences. We envision that this pipeline will provide a basis for efficient, end-to-end protein structure prediction through protein language modeling.},
author = {Costa, Allan dos Santos},
address = {Cambridge, Massachusetts},
title = {ChaperoNet : distillation of language model semantics to folded three-dimensional protein structures},
booktitle = { Master's Thesis },
publisher = {Massachusetts Institute of Technology},
title = {ChaperoNet : distillation of language model semantics to folded three-dimensional protein structures },
year = {2021},
}


@article {Costa2021.06.02.446809,
author = {Costa, Allan and Ponnapati, Manvitha and Jacobson, Joseph M. and Chatterjee, Pranam},
title = {Distillation of MSA Embeddings to Folded Protein Structures with Graph Transformers},
elocation-id = {2021.06.02.446809},
year = {2021},
doi = {10.1101/2021.06.02.446809},
publisher = {Cold Spring Harbor Laboratory},
abstract = {Determining the structure of proteins has been a long-standing goal in biology. Language models have been recently deployed to capture the evolutionary semantics of protein sequences. Enriched with multiple sequence alignments (MSA), these models can encode protein tertiary structure. In this work, we introduce an attention-based graph architecture that exploits MSA Transformer embeddings to directly produce three-dimensional folded structures from protein sequences. We envision that this pipeline will provide a basis for efficient, end-to-end protein structure prediction.Competing Interest StatementThe authors have declared no competing interest.},
URL = {https://www.biorxiv.org/content/early/2021/06/02/2021.06.02.446809},
eprint = {https://www.biorxiv.org/content/early/2021/06/02/2021.06.02.446809.full.pdf},
journal = {bioRxiv}
}

@misc{https://doi.org/10.48550/arxiv.2007.10784,
  doi = {10.48550/ARXIV.2007.10784},
  url = {https://arxiv.org/abs/2007.10784},
  author = {Costa, Allan and Dangovski, Rumen and Dugan, Owen and Kim, Samuel and Goyal, Pawan and Soljačić, Marin and Jacobson, Joseph},
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fast Neural Models for Symbolic Regression at Scale},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2004.07132,
  doi = {10.48550/ARXIV.2004.07132},
  url = {https://arxiv.org/abs/2004.07132},
  author = {Langenkamp, Max and Costa, Allan and Cheung, Chris},
  keywords = {Human-Computer Interaction (cs.HC), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hiring Fairly in the Age of Algorithms},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution 4.0 International}
}



@inproceedings{Costa_2019,
doi = {10.1109/aero.2019.8741572},
url = {https://doi.org/10.1109%2Faero.2019.8741572},
year = 2019,
month = {mar},
publisher = {{IEEE}},
author = {Allan Costa and Amira Abdel-Rahman and Benjamin Jenett and Neil Gershenfeld and Irina Kostitsyna and Kenneth Cheung},
title = {Algorithmic Approaches to Reconfigurable Assembly Systems},
booktitle = {2019 {IEEE} Aerospace Conference}
}
